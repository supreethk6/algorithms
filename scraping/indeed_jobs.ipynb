{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bs4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cae56718bae8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mselenium\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bs4'"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the initial HTML data:\n",
    "- Indeed had a standardised format for URL, which would make the web scraping easier. The URL was ‘indeed.com/jobs?’ followed by ‘q=job title’ & ‘l=location’ \n",
    "- This made tailoring the job title and location pretty easy. Then create a function that would take in job title and location as arguments, so that anybody could tailor the search\n",
    "- Using the **urlencode** function from **urllib** enabled to slot the arguments in to create the full url. Including ‘fromage=list’ and ‘sort=date’ within the URL, so that only the most recent jobs were displayed.\n",
    "- With the help of **BeautifulSoup**, extract the HTML and parse it appropriately.\n",
    "- And lastly, find the appropriate <div> that contained all of the job listings. One can find this by opening the URL (indeed.com/jobs?q=robotics+software+engineer&l=sunnyvale) and using the ‘Inspect’ element. With this, one could see that <td id=“resultsCol”> contained all of the job listings, so used soup.find(id=“resultsCol”) to select all of these jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indeed_jobs_div(job_title, location):\n",
    "    getVars = {'q':job_title, 'l':location, 'fromage':'last', 'sort':'data'}\n",
    "    url = ('https://www.indeed.com/jobs?' + urllib.parse.urlencode(gerVars))\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    job_soup = soup.find(id=\"resultsCol\")\n",
    "    return job_soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting job details\n",
    "After having the ‘soup’ of HTML containing all the job listings, the next step is to extract the information that is wanted, which were:\n",
    "- The job titles\n",
    "- The companies\n",
    "- The link to the full job profile\n",
    "- The date it is listed\n",
    "For each of these, again use Inspect to identify the appropriate section, and use the .find() function to identify them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_job_title_indeed(job_elem):\n",
    "    title_elem = job_elem.find('h2', class_='title')\n",
    "    title = title_elem.text.strip()\n",
    "    return title\n",
    "\n",
    "def extract_company_indeed(job_elem):\n",
    "    company_elem = job_elem.find('span', class_='company')\n",
    "    company = company_elem.text.strip()\n",
    "    return company\n",
    "\n",
    "def extract_link_indeed(job_elem):\n",
    "    link = job_eleme.find('a')['href']\n",
    "    link = 'www.indeed.com/' + link\n",
    "    return link\n",
    "\n",
    "def extract_date_indeed(job_elem):\n",
    "    date_elem = job_elem.find('span', class_='data')\n",
    "    data = date_elem.text.strip()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to Chris Lovejoy ^^^ implemented with the help of his blog."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
